# -*- coding: utf-8 -*-
"""Shakespeare_Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zjc8cajJYMa06o1vmacykqrQYzzRuXWh
"""

import tensorflow as tf
import tensorflow.keras as tfk
import numpy as np

shakespeare_url = "https://homl.info/shakespeare"
shakespeare_input = tfk.utils.get_file("shakespeare.txt",shakespeare_url)
with open(shakespeare_input) as input_file:
  shakespear_text = input_file.read()

len(shakespear_text)

tokenizer_for_shake = tfk.preprocessing.text.Tokenizer(char_level=True)
tokenizer_for_shake.fit_on_texts(shakespear_text)

max_id = len(tokenizer_for_shake.word_index)
print(tokenizer_for_shake.document_count)
print(len(tokenizer_for_shake.word_index))

shakespear_text_to_numbers = np.array(tokenizer_for_shake.texts_to_sequences(shakespear_text))-1

type(shakespear_text_to_numbers)

n_steps=100
train_dataset = tf.data.Dataset.from_tensor_slices(shakespear_text_to_numbers)
print(type(train_dataset))
for i in train_dataset:
  print(type(i))
  print(i.get_shape())
  print(i)
  break

time_sliced_input_windows = train_dataset.window(size=n_steps+1,shift=25,drop_remainder=True)
print(type(time_sliced_input_windows))
for i in time_sliced_input_windows:
  print(type(i))
  for k in i:
    print(type(k))
    print(k)
    break
  break

time_sliced_tensors = time_sliced_input_windows.flat_map(lambda window:window.batch(n_steps+1))
print(type(time_sliced_tensors))
for i in time_sliced_tensors:
  print(type(i))
  print(i.get_shape())
  break

batch_size=500
shuffeled_and_batched_dataset = time_sliced_tensors.shuffle(10000).batch(batch_size)

print(type(shuffeled_and_batched_dataset))
for i in shuffeled_and_batched_dataset:
  print(type(i))
  print(i.get_shape())
  break

time_sliced_x_y = shuffeled_and_batched_dataset.map(lambda window:(window[:,:-1],window[:,1:]))
print(type(time_sliced_x_y))
for i in time_sliced_x_y:
  print(type(i))
  print(len(i))
  print(type(i[0]))
  print(i[0].get_shape())
  print(type(i[1]))
  print(i[1].get_shape())
  break

one_hot_x_normal_y = time_sliced_x_y.map(lambda x,y:(tf.squeeze(tf.one_hot(x,depth=max_id),2),y))
for i in one_hot_x_normal_y:
  print(type(i))
  print(len(i))
  print(type(i[0]))
  print(i[0].get_shape())
  print(type(i[1]))
  print(i[1].get_shape())
  break

input = tfk.layers.Input(shape=(None,39))
first_rnn = tfk.layers.GRU(120,return_sequences=True,recurrent_dropout=0.2,dropout=0.2)(input)
second_rnn = tfk.layers.GRU(120,return_sequences=True,recurrent_dropout=0.2,dropout=0.2)(first_rnn)
final_dense = tfk.layers.TimeDistributed(tfk.layers.Dense(39,activation=tfk.activations.softmax))(second_rnn)

rnn_model = tfk.models.Model(inputs=[input],outputs=[final_dense])
rnn_model.compile(loss=tfk.losses.sparse_categorical_crossentropy,optimizer=tfk.optimizers.Adam(0.01))

rnn_model.summary()

one_hot_x_normal_y = one_hot_x_normal_y.prefetch(1)

for i in one_hot_x_normal_y:
  print(type(i))
  print(len(i))
  print(type(i[0]))
  print(i[0].get_shape())
  print(type(i[1]))
  print(i[1].get_shape())
  break

rnn_model.load_weights('Tute/2_RNN_Basics/models/6_shakesphere_rnn/weights/checkpoint')
rnn_model.fit(one_hot_x_normal_y,epochs=20)

rnn_model.save('entire_model_3/')

rnn_model.save_weights('weights_only_3/')

def preprocess_text(text):
  return tf.one_hot(np.array(tokenizer_for_shake.texts_to_sequences(text))-1,max_id)

X_new = [""]
processed = preprocess_text(X_new)
Y_pred=np.argmax(rnn_model.predict(processed),axis=-1)
tokenizer_for_shake.sequences_to_texts(Y_pred + 1)[0][::-1]